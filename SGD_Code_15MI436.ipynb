{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stochastic gradient descent, we update our values after looking at each item in the training set, so that we can start making progress right away. Recall the linear regression example above. In that example, we calculated the gradient for each of the two theta values as follows:\n",
    "\n",
    "\n",
    "∂/∂𝜃o𝐽(𝜃o,𝜃1)=1/𝑚∑𝑖=1 to 𝑚(ℎ𝜃(𝑥𝑖)−𝑦𝑖) \n",
    "\n",
    "∂/∂𝜃1𝐽(𝜃o,𝜃1)=1/𝑚∑𝑖=1 to 𝑚((ℎ𝜃(𝑥𝑖)−𝑦𝑖)⋅𝑥𝑖)\n",
    "\n",
    "Where  ℎ𝜃(𝑥)=𝜃o+𝜃1𝑥 \n",
    "\n",
    "When the sample data had 15 data points as in the example above, calculating the gradient was not very costly. But for very large data sets, this would not be the case. So instead, we consider a stochastic gradient descent algorithm for simple linear regression such as the following, where m is the size of the data set:\n",
    "\n",
    "    1:   Randomly shuffle the data set \n",
    "    2:   for k = 0, 1, 2, ... do \n",
    "    3:       for i = 1 to m do \n",
    "    4:             [𝜃1   [𝜃1   −  𝛼[2(ℎ𝜃(𝑥𝑖)−𝑦𝑖)\n",
    "                   𝜃2]=  𝜃2]        2𝑥𝑖(ℎ𝜃(𝑥𝑖)−𝑦𝑖)]    \n",
    "    5:       end for \n",
    "    6:   end for\n",
    "\n",
    "Typically, with stochastic gradient descent, you will run through the entire data set 1 to 10 times (see value for k in line 2 of the pseudocode above), depending on how fast the data is converging and how large the data set is.\n",
    "\n",
    "With this algorithm though, we can make progress right away and continue to make progress as we go through the data set. Therefore, stochastic gradient descent is often preferred when dealing with large data sets.\n",
    "\n",
    "Unlike gradient descent, stochastic gradient descent will tend to oscillate near a minimum value rather than continuously getting closer. It may never actually converge to the minimum though. One way around this is to slowly decrease the step size  𝛼  as the algorithm runs. However, this is less common than using a fixed  𝛼 .\n",
    "\n",
    "Let's look at another example where we illustrate the use of stochastic gradient descent for linear regression. In the example below, we'll create a set of 500,000 points around the line  𝑦= 2x+17+$ , for values of x between 0 and 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x*3+81+np.random.randn(len(x))*10\n",
    "\n",
    "x = np.random.random(100000)*100\n",
    "y = f(x) \n",
    "m = len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's randomly shuffle around our dataset. Note that in this example, this step isn't strictly necessary since the data is already in a random order. However, that obviously may not always be the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "x_shuf = []\n",
    "y_shuf = []\n",
    "index_shuf = range(len(x))\n",
    "shuffle(index_shuf)\n",
    "for i in index_shuf:\n",
    "    x_shuf.append(x[i])\n",
    "    y_shuf.append(y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll setup our h function and our cost function, which we will use to check how the value is improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = lambda theta_0,theta_1,x: theta_0 + theta_1*x\n",
    "cost = lambda theta_0,theta_1, x_i, y_i: 0.5*(h(theta_0,theta_1,x_i)-y_i)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run our stochastic gradient descent algorithm. To see it's progress, we'll take a cost measurement at every step. Every 10,000 steps, we'll get an average cost from the last 10,000 steps and then append that to our cost_list variable. We will run through the entire list 10 times here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_old = np.array([0.,0.])\n",
    "theta_new = np.array([1.,1.]) # The algorithm starts at [1,1]\n",
    "n_k = 0.000005 # step size\n",
    "\n",
    "iter_num = 0\n",
    "s_k = np.array([float(\"inf\"),float(\"inf\")])\n",
    "sum_cost = 0\n",
    "cost_list = []\n",
    "\n",
    "for j in range(10):\n",
    "    for i in range(m):\n",
    "        iter_num += 1\n",
    "        theta_old = theta_new\n",
    "        s_k[0] = (h(theta_old[0],theta_old[1],x[i])-y[i])\n",
    "        s_k[1] = (h(theta_old[0],theta_old[1],x[i])-y[i])*x[i]\n",
    "        s_k = (-1)*s_k\n",
    "        theta_new = theta_old + n_k * s_k\n",
    "        sum_cost += cost(theta_old[0],theta_old[1],x[i],y[i])\n",
    "        if (i+1) % 10000 == 0:\n",
    "            cost_list.append(sum_cost/10000.0)\n",
    "            sum_cost = 0   \n",
    "            \n",
    "print \"Local minimum occurs where:\"\n",
    "print \"theta_0 =\", theta_new[0] \n",
    "print \"theta_1 =\", theta_new[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local minimum occurs where:\n",
    "theta_0 = 16.97365544638909\n",
    "theta_1 = 1.993228510626234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our values for  𝜃0  and  𝜃1  are close to their true values of 17 and 2.\n",
    "\n",
    "Now, we plot our cost versus the number of iterations. As you can see, the cost goes down quickly at first, but starts to level off as we go through more iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.arange(len(cost_list))*10000\n",
    "plt.plot(iterations,cost_list)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"avg cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](1.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
